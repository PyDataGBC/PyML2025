{"cells":[{"cell_type":"markdown","source":["#LabWeek9: Prevent Overfitting - Part I"],"metadata":{"id":"EvwsaaC-zho0"}},{"cell_type":"markdown","metadata":{"id":"kj231EE4lWug"},"source":["## Importing Dataset: IMDB"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"19gzQeyMlWug"},"outputs":[],"source":["from keras.datasets import imdb\n","import numpy as np\n","\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n","\n","def vectorize_sequences(sequences, dimension=10000):\n","    # Create an all-zero matrix of shape (len(sequences), dimension)\n","    results = np.zeros((len(sequences), dimension))\n","    for i, sequence in enumerate(sequences):\n","        for j in sequence:\n","            results[i, j] = 1.  # set specific indices of results[i] to 1s\n","    return results\n","\n","# Our vectorized training data\n","x_train = vectorize_sequences(train_data)\n","# Our vectorized test data\n","x_test = vectorize_sequences(test_data)\n","# Our vectorized labels\n","y_train = np.asarray(train_labels).astype('float32')\n","y_test = np.asarray(test_labels).astype('float32')"]},{"cell_type":"markdown","source":["**Note:** We first encountered this dataset in `LabWeek5`. You can refer to that notebook for clarification on the text-to-numbers decoding process. Through vectorizing process we turn text data into vectors."],"metadata":{"id":"SL2hI5sppEyg"}},{"cell_type":"markdown","source":["**Perform some sanity checks on the dataset:** For example, find the `shape` of the dataset and print samples to make sense of the data."],"metadata":{"id":"YZ83Fe0jnMyB"}},{"cell_type":"code","source":["#TODO: your code here"],"metadata":{"id":"oE7R9euJnJ0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d-WNlh6RlWug"},"source":["## Fighting overfitting -  Reducing the network's size\n","\n","\n","The general workflow to find an appropriate model size is to start with relatively few layers and\n","parameters, and start increasing the size of the layers or adding new layers until you see diminishing returns with regard to the\n","validation loss.\n","\n"]},{"cell_type":"markdown","source":["**Note:** we use `test set` as our `validation set`."],"metadata":{"id":"acSrlchQqEkd"}},{"cell_type":"markdown","source":["**Complete the following code for `original model` architecture**"],"metadata":{"id":"9s8d5jODsb3H"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"HViJio6llWuh"},"outputs":[],"source":["from keras import models\n","from keras import layers\n","\n","original_model = models.Sequential()\n","original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n","#TODO: add one hidden layer with 16 units with proper activation function\n","\n","#TODO: add the output layer. Remember our task is binary classificaion, whether a review is 0:negative or 1:positive.\n","\n","original_model.compile(optimizer='rmsprop',\n","                       loss='binary_crossentropy',\n","                       metrics=['acc'])"]},{"cell_type":"markdown","metadata":{"id":"jLrhrdetlWuh"},"source":["**Define another model with fewer parameters. 4 units in each hidden layer.**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Mhz_k7f6lWuh"},"outputs":[],"source":["smaller_model = models.Sequential()\n","#TODO: add hidden/ouput layers and compile the smaller model"]},{"cell_type":"markdown","metadata":{"id":"M07ol9R-lWuh"},"source":["\n","**Train (1) original_model and (2) smaller_model seperately for `epochs=20` and `batch_size=512`. Also pass `x_test` and `y_test` as `validatio_data`.**"]},{"cell_type":"code","source":["#TODO: your code here for training original_model\n","original_hist = ..."],"metadata":{"id":"5QB8d1iLtpkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TODO: your code here for training original_model\n","smaller_model_hist = ..."],"metadata":{"id":"5ujpGtuvtyb1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Plot `val_loss` vs. epochs for both models in one plot**"],"metadata":{"id":"RYgKMNF8u9B_"}},{"cell_type":"code","source":["#TODO: your code here"],"metadata":{"id":"pmbSPySFvIkN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Questions:** Which model overfits earlier? How can you see the size of the model on overfitting?"],"metadata":{"id":"xLYftwtuvwCl"}},{"cell_type":"markdown","source":["**Answer:** TODO"],"metadata":{"id":"BtM87A1fwju-"}},{"cell_type":"markdown","source":["**Now define a very large model (e.g., with 512 units in hidden layer) and compare the `loss` and `val_loss` of this big model and the original model.**"],"metadata":{"id":"TlS-jTVEw4EQ"}},{"cell_type":"code","source":["#your code here"],"metadata":{"id":"OrBI9LGpxe-7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Discuss the results**"],"metadata":{"id":"BW0vJsK7xoIn"}},{"cell_type":"markdown","source":["**Answer:** TODO"],"metadata":{"id":"JnjkvgHZxsho"}},{"cell_type":"markdown","metadata":{"id":"feDoVK-ulWuj"},"source":["## Fighting overfitting - Adding weight regularization\n","\n","\n","A common way to mitigate overfitting is to put constraints on the complexity\n","of a network by forcing its weights to only take small values. This is called\n","\"weight regularization\", and it is done by adding to the loss function of the network a _cost_ associated with having large weights. This\n","cost comes in two flavors:\n","\n","* L1 regularization, where the cost added is proportional to the _absolute value of the weights coefficients_ (i.e. to what is called the\n","\"L1 norm\" of the weights).\n","* L2 regularization, where the cost added is proportional to the _square of the value of the weights coefficients_ (i.e. to what is called\n","the \"L2 norm\" of the weights). L2 regularization is also called _weight decay_ in the context of neural networks. Don't let the different\n","name confuse you: weight decay is mathematically the exact same as L2 regularization.\n","\n","In Keras, weight regularization is added by passing _weight regularizer instances_ to layers as keyword arguments.\n","\n","\n","**Complere the model below**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"eC3-OjMclWuj"},"outputs":[],"source":["from keras import regularizers\n","\n","l2_model = models.Sequential()\n","l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n","                          activation='relu', input_shape=(10000,)))\n","\n","#TODO: Add a dense layer with 16 unuts with L2 regularization\n","\n","l2_model.add(layers.Dense(1, activation='sigmoid'))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"OaCiCrxNlWuj"},"outputs":[],"source":["l2_model.compile(optimizer='rmsprop',\n","                 loss='binary_crossentropy',\n","                 metrics=['acc'])"]},{"cell_type":"markdown","metadata":{"id":"u3fi3YEvlWuj"},"source":["`l2(0.001)` means that every coefficient in the weight matrix of the layer will add `0.001 * weight_coefficient_value` to the total loss of\n","the network. Note that because this penalty is _only added at training time_, the loss for this network will be much higher at training\n","than at test time.\n","\n"]},{"cell_type":"markdown","source":["**Train the model with regularization and compare the validation loss between this model and the `original_model`.**"],"metadata":{"id":"Je1FLLMJJnii"}},{"cell_type":"code","source":["#TODO: your code here"],"metadata":{"id":"TOA9BjOoJ_Nw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KW5NIFdflWuj"},"source":["**Note:** As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"nIBko9i_lWuj"},"outputs":[],"source":["from keras import regularizers\n","\n","# L1 regularization\n","regularizers.l1(0.001)\n","\n","# L1 and L2 regularization at the same time\n","regularizers.l1_l2(l1=0.001, l2=0.001)"]},{"cell_type":"markdown","source":["## Recap"],"metadata":{"id":"6C_YYIRgKRpg"}},{"cell_type":"markdown","metadata":{"id":"ZRbCd03LyHEF"},"source":["To recap: here the most common ways to prevent overfitting in neural networks:\n","\n","* Getting more training data.\n","* Reducing the capacity of the network.\n","* Adding weight regularization.\n","* Adding dropout. (next session)"]},{"cell_type":"markdown","source":["**Observation 1:** The bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be\n","able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large\n","difference between the training and validation loss).\n","\n","**Observation 2:** The model with L2 regularization has become much more resistant to overfitting than the reference model,\n","even though both models have the same number of parameters."],"metadata":{"id":"ZvGql3lUKa7h"}},{"cell_type":"code","source":[],"metadata":{"id":"uUGQbuanyTmh"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}