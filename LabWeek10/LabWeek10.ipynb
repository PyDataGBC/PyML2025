{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Lab - Week 10 - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to reduce overfitting in neural networks - all come at a cost\n",
    " - Getting more training data\n",
    " - Reducing the capacity of the network\n",
    " - Adding weight regularization (last week's lab)\n",
    " - Adding dropout (the subject for this week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical training and validation loss over time\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*0VWDpLIRcMTssDf-zyOR4w.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using dropout to reduce overfitting\n",
    "\n",
    "Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his \n",
    "students at the University of Toronto. Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. setting to zero) a number of \n",
    "output features of the layer during training. Let's say a given layer would normally have returned a vector `[0.2, 0.5, 1.3, 0.8, 1.1]` for a \n",
    "given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. `[0, 0.5, \n",
    "1.3, 0, 1.1]`. The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test \n",
    "time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to \n",
    "balance for the fact that more units are active than at training time.\n",
    "\n",
    "![dropout](https://cdn-images-1.medium.com/max/1600/1*iWQzxhVlvadk6VAJjsgXgg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras you can introduce dropout in a network via the `Dropout` layer, which gets applied to the output of layer right before it, e.g.:\n",
    "```python\n",
    "model.add(layers.Dropout(0.5))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Load and Prepare Data - same as last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n",
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Simple Neural Network Model\n",
    "Objective: Create a basic neural network model using Keras.\n",
    "\n",
    "**Questions**\n",
    "- What type of classification problem are we solving?\n",
    "- How many neurons should there be in the output layer?\n",
    "- What should be the activation function of hte output layer?\n",
    "- What loss function should be used? \n",
    "- What activation function should be used on the hidden layers?\n",
    "\n",
    "\n",
    "Build a sequential model with ne dense layer with 8 units and train it for 20 epochs. \n",
    "\n",
    "Complete the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix this code\n",
    "# Add a single hidden layer with 8 units and relu activation\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add ...  \n",
    "\n",
    "# Train the model\n",
    "hist_a = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", evaluation_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Visualize Training and Validation Performance\n",
    "\n",
    "The function below can be used later to plot the loss and accuracy from the model training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# colors will be used to plot the different models below\n",
    "colors = ['blue', 'red', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "\n",
    "def plot_history(history, color='blue', prefix=\"\"):\n",
    "    history_dict = history.history\n",
    "    loss_values = history_dict[\"loss\"]\n",
    "    val_loss_values = history_dict[\"val_loss\"]\n",
    "    epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "    # Make a figure with two subplots side by side\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Plot the loss\n",
    "    plt.plot(epochs, loss_values, \"o\",  color=color, label=prefix + \" Training loss\")\n",
    "    plt.plot(epochs, val_loss_values, color=color, label=prefix + \" Validation loss\")\n",
    "    plt.title(\"Training and validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(framealpha=0.5)\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot the accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    acc_values = history_dict[\"acc\"]\n",
    "    val_acc_values = history_dict[\"val_acc\"]\n",
    "    # Skip plotting the training accuracy, it makes the plot harder to read\n",
    "    # plt.plot(epochs, acc_values, \"o\", color=color , label=prefix + \" Training accuracy\")\n",
    "    plt.plot(epochs, val_acc_values, color=color, label=prefix + \" Validation accuracy\")\n",
    "    plt.title(\"Training and validation accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(framealpha=0.5)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the plotting function\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_history(hist_a, color='blue', prefix=\"Small model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Add Dropout Regularization\n",
    "Objective: Experiment with dropout layers in the model.\n",
    "\n",
    "**Instructions:**\n",
    "1. Add another hidden layer with 8 units\n",
    "1. Add dropout layers after each hidden layer with a dropout rate of 0.3.\n",
    "1. Train the model and observe the changes in accuracy.\n",
    "\n",
    "Complete the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a model with 30% Dropout\n",
    "drp_model = models.Sequential()\n",
    "...\n",
    "\n",
    "drp_history = drp_model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_history(drp_history, color='red', prefix=\"Dropout 0.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Test Different Dropout Rates\n",
    "Objective: Compare the model's performance with varying dropout rates.\n",
    "\n",
    "**Instructions:**\n",
    "1. Write a loop to create and train models with dropout rates of `[0, 0.2, 0.3, 0.4, 0.5]`.\n",
    "2. For each model, record the history to plot it later\n",
    "\n",
    "\n",
    "Complete the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try different dropout rates\n",
    "\n",
    "rates = [0, 0.2, 0.3, 0.4, 0.5]\n",
    "hists = []\n",
    "\n",
    "for rate in rates:\n",
    "    print(f\"Training model with dropout rate: {rate}\")\n",
    "    model = ????\n",
    "    \n",
    "    hist = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "    hists.append(hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for i, hist in enumerate(hists):\n",
    "    plot_history(hist, colors[i], prefix=str(rates[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Experiment with dropout location\n",
    "Objective: Compare the model's performance with dropout placed after different layers\n",
    "\n",
    "**Instructions:**\n",
    "Create 3 models with a single dropout \"layer\" placed:\n",
    "1. Before the first dense layer only\n",
    "2. After the first dense layer only\n",
    "3. After the second dense layer only\n",
    "\n",
    "\n",
    "\n",
    "Complete the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
